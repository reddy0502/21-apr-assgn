{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "14b3f422-826d-48bf-9bb0-917ddc77b7bd",
   "metadata": {},
   "source": [
    "1ans:\n",
    "\n",
    "Euclidean distance is calculated as the square root of the sum of the squared differences between each feature of the two data points. It represents the straight-line distance between two points in a 2-dimensional space, and can be extended to higher dimensions.\n",
    "\n",
    "Manhattan distance, also known as L1 distance or taxicab distance, is calculated as the sum of the absolute differences between each feature of the two data points. It represents the distance between two points if a person were to walk along the city blocks to get from one point to another in a grid-like city.\n",
    "\n",
    "The choice of distance metric can affect the performance of a KNN classifier or regressor, as it impacts how the algorithm measures similarity between data points. In general, Euclidean distance tends to work well when the features have continuous values and there are no strong correlations between them. On the other hand, Manhattan distance may work better when the features have categorical or ordinal values, or when there are strong correlations between them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aac7176-1b41-4179-a720-81a9964ffed0",
   "metadata": {},
   "source": [
    "2ans:\n",
    "\n",
    "Choosing the optimal value of k for a KNN classifier or regressor is important for obtaining good performance. The value of k determines the number of neighbors that are considered for making predictions, and it can have a significant impact on the accuracy of the model.\n",
    "\n",
    "There are several techniques that can be used to determine the optimal k value for KNN:\n",
    "\n",
    "Grid search: This involves training and evaluating the model with different values of k, and selecting the value of k that results in the highest accuracy or lowest error rate on a validation set. The range of k values to try can be specified manually or using an automated search algorithm.\n",
    "\n",
    "Cross-validation: This involves splitting the data into multiple folds, training the model with different values of k on each fold, and evaluating the performance of the model on a validation set. The value of k that results in the highest average accuracy or lowest average error rate across all folds can be selected as the optimal value.\n",
    "\n",
    "Elbow method: This involves plotting the accuracy or error rate of the model as a function of k, and selecting the value of k at the \"elbow\" point where the rate of improvement starts to slow down. This can be a visual way to estimate the optimal k value."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65f7dcf7-b34c-418e-ba66-75ac994b9e0b",
   "metadata": {},
   "source": [
    "3ans:\n",
    "\n",
    "The choice of distance metric can have a significant impact on the performance of a KNN classifier or regressor. KNN relies on the distance between data points to make predictions, so the distance metric used directly affects the quality of the predictions.\n",
    "\n",
    "Here are some common distance metrics used in KNN:\n",
    "\n",
    "Euclidean distance: This is the most common distance metric used in KNN. It measures the straight-line distance between two points in a Euclidean space. It works well when the data is evenly distributed and there are no significant outliers.\n",
    "\n",
    "Manhattan distance: This metric measures the distance between two points as the sum of the absolute differences of their coordinates. It is better suited for cases where the data is not evenly distributed and contains outliers.\n",
    "\n",
    "Cosine similarity: This metric measures the cosine of the angle between two vectors in a high-dimensional space. It works well for cases where the data is in a high-dimensional space, such as natural language processing tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f29048e-e85c-4813-83c4-2bbc36c630fe",
   "metadata": {},
   "source": [
    "4ans:\n",
    "\n",
    " Some common hyperparameters include:\n",
    "\n",
    "K: This is the number of nearest neighbors to consider when making a prediction. A larger K value will result in a smoother decision boundary but may also lead to overfitting. A smaller K value may result in a more accurate but less stable prediction.\n",
    "\n",
    "Distance metric: This is the distance metric used to measure the similarity between data points. As discussed in the previous answer, there are several distance metrics to choose from, and the choice of metric can have a significant impact on model performance.\n",
    "\n",
    "Weighting: This determines how the K nearest neighbors are weighted when making a prediction. Uniform weighting treats all neighbors equally, while distance weighting gives more weight to closer neighbors.\n",
    "\n",
    "To tune these hyperparameters, one common approach is to use a grid search or random search over a range of possible values. This involves evaluating the model's performance on a validation set for each combination of hyperparameters and selecting the combination that gives the best performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43dd4dc4-cfaa-48d7-86c9-c71369dcba8e",
   "metadata": {},
   "source": [
    "5ans:\n",
    "\n",
    "The size of the training set can have a significant impact on the performance of a KNN classifier or regressor. In general, a larger training set will provide more representative samples of the population, leading to more accurate predictions. However, as the size of the training set grows, the computational cost of KNN also increases. In addition, having too much training data may lead to overfitting, where the model becomes too specialized to the training data and performs poorly on new data.\n",
    "\n",
    "To optimize the size of the training set, a common approach is to use a learning curve. A learning curve plots the performance of the model as a function of the training set size. By analyzing the learning curve, we can determine the point of diminishing returns, where increasing the size of the training set no longer leads to significant improvements in performance. This can help us determine the optimal size of the training set for a given task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7530fe1e-2e07-4878-82f6-868136fe80fa",
   "metadata": {},
   "source": [
    "6ans:\n",
    "\n",
    "Computationally expensive: KNN has to compute the distance between the test data and all training data, which can be computationally expensive when the training set is large. This can limit its scalability and make it less suitable for real-time or online applications.\n",
    "\n",
    "Sensitivity to feature scaling: KNN is sensitive to the scaling of the features. Features that have a larger range or magnitude may have a larger impact on the distance metric, leading to biased results. Feature scaling techniques such as normalization or standardization can help to overcome this issue.\n",
    "\n",
    "Sensitivity to noisy or irrelevant features: KNN may perform poorly when the dataset contains noisy or irrelevant features. These features can lead to an inaccurate distance metric and affect the quality of the predictions.\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
